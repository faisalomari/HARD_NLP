{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Filtering the Reviews Data\n",
    "This code loads the reviews dataset, filters reviews with specific ratings (1, 2, 4, 5), and classifies the reviews as 'Positive' or 'Negative' based on the rating. Finally, it selects 10 random reviews for display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reviews data with the correct encoding\n",
    "file_path = 'HARD-Arabic-Dataset-master/data/balanced-reviews.txt'  # Update with your actual file path\n",
    "df = pd.read_csv(file_path, sep='\\t', names=['no', 'Hotel name', 'rating', 'user type', 'room type', 'nights', 'review'], skiprows=1, encoding='utf-16')\n",
    "\n",
    "# Continue with filtering and classifying as before\n",
    "df_filtered = df[df['rating'].isin([1, 2, 4, 5])]\n",
    "selected_reviews = df_filtered.sample(n=10)\n",
    "\n",
    "def classify_review(row):\n",
    "    if row['rating'] in [4, 5]:\n",
    "        return 'Positive'\n",
    "    elif row['rating'] in [1, 2]:\n",
    "        return 'Negative'\n",
    "\n",
    "selected_reviews['classification'] = selected_reviews.apply(classify_review, axis=1)\n",
    "selected_reviews[['no','review', 'rating', 'classification']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, Filter, and Split Reviews Dataset\n",
    "This code loads the reviews dataset, filters reviews based on specific ratings (1, 2, 4, and 5), and limits the total number of reviews to a maximum of 1000. It classifies the reviews as 'Positive' or 'Negative' based on their rating, saves the classified data to a CSV file, and then splits the dataset into 70% training and 30% testing data, saving them as separate CSV files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum number of reviews to use\n",
    "max_reviews = 1000\n",
    "\n",
    "# Load the reviews data with the correct encoding\n",
    "file_path = 'HARD-Arabic-Dataset-master/data/balanced-reviews.txt'  # Update with your actual file path\n",
    "df = pd.read_csv(file_path, sep='\\t', names=['no', 'Hotel name', 'rating', 'user type', 'room type', 'nights', 'review'], skiprows=1, encoding='utf-16')\n",
    "\n",
    "# Filter reviews with ratings 1, 2, 4, and 5\n",
    "df_filtered = df[df['rating'].isin([1, 2, 4, 5])]\n",
    "\n",
    "# Limit the number of reviews if max_reviews is set\n",
    "if max_reviews is not None:\n",
    "    df_filtered = df_filtered.head(max_reviews)\n",
    "\n",
    "# Function to classify reviews based on the rating\n",
    "def classify_review(row):\n",
    "    if row['rating'] in [4, 5]:\n",
    "        return 'Positive'\n",
    "    elif row['rating'] in [1, 2]:\n",
    "        return 'Negative'\n",
    "\n",
    "# Apply classification function to the dataset\n",
    "df_filtered['classification'] = df_filtered.apply(classify_review, axis=1)\n",
    "\n",
    "# Select necessary columns (id, review text, classification)\n",
    "df_classified = df_filtered[['no', 'review', 'classification']]\n",
    "\n",
    "# Save the classified reviews to a CSV file\n",
    "output_file = 'classified.csv'\n",
    "df_classified.to_csv(output_file, index=False, encoding='utf-16')\n",
    "\n",
    "# Split the data into 70% training and 30% testing\n",
    "train_data, test_data = train_test_split(df_classified, test_size=0.3, random_state=42)\n",
    "\n",
    "# Save the split datasets\n",
    "train_data.to_csv('train_reviews.csv', index=False, encoding='utf-16')\n",
    "test_data.to_csv('test_reviews.csv', index=False, encoding='utf-16')\n",
    "\n",
    "print(f\"Data saved to {output_file}, and split into train_reviews.csv and test_reviews.csv.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, Tokenize, and Train Arabic BERT Model\n",
    "This code loads the training and testing datasets, applies label encoding for 'Positive' and 'Negative' reviews, and tokenizes the review texts using a pre-trained Arabic BERT tokenizer. The data is then converted into Hugging Face dataset format. A BERT model for binary classification is loaded, training arguments are set, and a `Trainer` is defined for training and evaluating the model. The model is trained and evaluated, and then saved for future use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classified train and test data\n",
    "train_file = 'train_reviews.csv'\n",
    "test_file = 'test_reviews.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_file, encoding='utf-16')\n",
    "test_df = pd.read_csv(test_file, encoding='utf-16')\n",
    "\n",
    "# Labeling the data (1 for Positive, 0 for Negative)\n",
    "train_df['label'] = train_df['classification'].apply(lambda x: 1 if x == 'Positive' else 0)\n",
    "test_df['label'] = test_df['classification'].apply(lambda x: 1 if x == 'Positive' else 0)\n",
    "\n",
    "# Split texts and labels\n",
    "train_texts = train_df['review'].tolist()\n",
    "train_labels = train_df['label'].tolist()\n",
    "test_texts = test_df['review'].tolist()\n",
    "test_labels = test_df['label'].tolist()\n",
    "\n",
    "# Load pre-trained tokenizer for Arabic BERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv02\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Create Hugging Face datasets\n",
    "train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})\n",
    "test_dataset = Dataset.from_dict({'text': test_texts, 'label': test_labels})\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load BERT model for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"aubmindlab/bert-base-arabertv02\", num_labels=2)\n",
    "\n",
    "# Define accuracy metric function\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)  # Get the predicted class\n",
    "    acc = accuracy_score(p.label_ids, preds)  # Calculate accuracy\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# Create a data collator that dynamically pads inputs\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics  # Add the metrics function here\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()\n",
    "\n",
    "# Save the model for future use\n",
    "model.save_pretrained(\"arabic_bert_review_classifier\")\n",
    "tokenizer.save_pretrained(\"arabic_bert_review_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data, Predict Classifications, and Display Confusion Matrix\n",
    "This code block loads the dataset of hotel reviews, filters them based on their ratings (1, 2 for 'Negative' and 4, 5 for 'Positive'), and applies a pre-trained Arabic BERT model to predict whether each review is 'Positive' or 'Negative'. It calculates the accuracy of the model's predictions, generates a confusion matrix to visualize the classification performance, and saves the final output with the actual and predicted classifications to a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reviews data with the correct encoding\n",
    "file_path = 'HARD-Arabic-Dataset-master/data/balanced-reviews.txt'  # Update with your actual file path\n",
    "df = pd.read_csv(file_path, sep='\\t', names=['no', 'Hotel name', 'rating', 'user type', 'room type', 'nights', 'review'], skiprows=1, encoding='utf-16')\n",
    "\n",
    "# Filter reviews with ratings 1, 2, 4, and 5\n",
    "df_filtered = df[df['rating'].isin([1, 2, 4, 5])]\n",
    "\n",
    "# Limit the number of reviews to 1000\n",
    "# df_filtered = df_filtered.head(1000)\n",
    "\n",
    "# Function to classify reviews based on the rating\n",
    "def classify_review(row):\n",
    "    if row['rating'] in [4, 5]:\n",
    "        return 'Positive'\n",
    "    elif row['rating'] in [1, 2]:\n",
    "        return 'Negative'\n",
    "\n",
    "# Apply classification function to the dataset\n",
    "df_filtered['classification'] = df_filtered.apply(classify_review, axis=1)\n",
    "\n",
    "# Load pre-trained tokenizer and model for Arabic BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv02\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"arabic_bert_review_classifier\")  # Load the trained model\n",
    "\n",
    "# Use GPU if available for faster performance\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to predict classifications in batches with progress bar\n",
    "def predict_in_batches(reviews, batch_size):\n",
    "    predictions = []\n",
    "    for i in tqdm(range(0, len(reviews), batch_size), desc=\"Processing Batches\", unit=\"batch\"):\n",
    "        batch_reviews = reviews[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_reviews, padding='max_length', truncation=True, return_tensors='pt', max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            preds = np.argmax(outputs.logits.cpu().numpy(), axis=1)  # Move to CPU before converting to NumPy\n",
    "            predictions.extend(preds)\n",
    "    return predictions\n",
    "\n",
    "# Predict classifications for all reviews in batches\n",
    "predictions = predict_in_batches(df_filtered['review'].tolist(), batch_size=64)\n",
    "\n",
    "# Map the predictions to the corresponding labels\n",
    "df_filtered['model_classification'] = ['Positive' if pred == 1 else 'Negative' for pred in predictions]\n",
    "df_filtered['model_correct'] = df_filtered['classification'] == df_filtered['model_classification']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = df_filtered['model_correct'].mean() * 100\n",
    "print(f\"Model Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "y_true = df_filtered['classification']\n",
    "y_pred = df_filtered['model_classification']\n",
    "cm = confusion_matrix(y_true, y_pred, labels=['Positive', 'Negative'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Positive', 'Negative'], yticklabels=['Positive', 'Negative'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Select necessary columns for the final output\n",
    "output_df = df_filtered[['no', 'Hotel name', 'rating', 'user type', 'room type', 'nights', 'review', 'classification', 'model_classification', 'model_correct']]\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "output_file = 'model_predictions.csv'\n",
    "output_df.to_csv(output_file, index=False, encoding='utf-16')\n",
    "\n",
    "print(f\"Predictions saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Examples of True Positive, True Negative, False Positive, and False Negative\n",
    "This block defines conditions for four classification outcomes: True Positive, True Negative, False Positive, and False Negative. It then selects 5 review examples from each category and prints their review ID, rating, review text, model's predicted classification, and the true classification for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conditions for True Positive, True Negative, False Positive, and False Negative\n",
    "true_positive = df_filtered[(df_filtered['classification'] == 'Positive') & (df_filtered['model_classification'] == 'Positive')]\n",
    "true_negative = df_filtered[(df_filtered['classification'] == 'Negative') & (df_filtered['model_classification'] == 'Negative')]\n",
    "false_positive = df_filtered[(df_filtered['classification'] == 'Negative') & (df_filtered['model_classification'] == 'Positive')]\n",
    "false_negative = df_filtered[(df_filtered['classification'] == 'Positive') & (df_filtered['model_classification'] == 'Negative')]\n",
    "\n",
    "# Function to display 5 examples for each case\n",
    "def display_examples(df, case_name):\n",
    "    print(f\"\\n{case_name} Examples:\")\n",
    "    for index, row in df.head(5).iterrows():\n",
    "        print(f\"Review ID: {row['no']}\")\n",
    "        print(f\"Rating: {row['rating']}\")\n",
    "        print(f\"Review: {row['review']}\")\n",
    "        print(f\"Model Classification: {row['model_classification']}\")\n",
    "        print(f\"True Classification: {row['classification']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Display 5 examples of each case\n",
    "display_examples(true_positive, \"True Positive\")\n",
    "display_examples(true_negative, \"True Negative\")\n",
    "display_examples(false_positive, \"False Positive\")\n",
    "display_examples(false_negative, \"False Negative\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
